{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6001216",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Ce rapport présente une analyse des risques détectés sur le lieu de travail à partir de données multi-modales (images, détections, météo, réglementation).\n",
    "\n",
    "Ce fichier servira pour l’étape de préparation des données et l’étape d’entraînement, principalement en phase de préproduction. Une structure différente sera ensuite utilisée pour les besoins de la production.\n",
    "\n",
    "### Données utilisées\n",
    "\n",
    "    - Images panoramiques HD\n",
    "    - Détections de personnes\n",
    "    - Indications de risques\n",
    "    - Météo\n",
    "\n",
    "Voici un exemple de l’image ainsi qu’une description JSON des informations liées à cette image:\n",
    "![Alt text](../assets/images_EST-1/654756511_999f6d8b-1331-4779-84e9-36b74bd21441.jpg \"a sample image\")\n",
    "\n",
    "Extrait d'information json:\n",
    "```\n",
    "\"654756511_999f6d8b-1331-4779-84e9-36b74bd21441.jpg\": {\n",
    "      \"photo_id\": 654756511,\n",
    "      \"photoset_id\": 1374225,\n",
    "      \"image_shooting\": \"2025:06:16 05:50:07\",\n",
    "      \"stitch_from\": \"stitching-tests/16db54d2-4541-4c48-8f34-68cef563cf06/3093724140/stitched/999f6d8b-1331-4779-84e9-36b74bd21441.jpg\",\n",
    "      \"detections\": [\n",
    "        {\n",
    "          \"score\": 0.1710205078125,\n",
    "          \"label\": \"person\",\n",
    "          \"bounding_box_start_x\": 0.9674935340881348,\n",
    "          \"bounding_box_end_x\": 0.9687273502349854,\n",
    "          \"bounding_box_start_y\": 0.6455875039100647,\n",
    "          \"bounding_box_end_y\": 0.6539404988288879,\n",
    "          \"attributes\": {\n",
    "            \"has_high_vis_pants\": 0.0,\n",
    "            \"has_hard_hat\": 0.0,\n",
    "            \"has_high_vis_vest\": 0.0,\n",
    "            \"no_ppe\": 0.0,\n",
    "            \"two_or_more\": 0.0\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"score\": 0.25830078125,\n",
    "          \"label\": \"bird\",\n",
    "          \"bounding_box_start_x\": 0.5115770101547241,\n",
    "          \"bounding_box_end_x\": 0.5254966020584106,\n",
    "          \"bounding_box_start_y\": 0.6345036625862122,\n",
    "          \"bounding_box_end_y\": 0.6598840355873108\n",
    "        },...}\n",
    "```\n",
    "\n",
    "L’idée de ce projet est de développer un modèle LLM pour la gestion des risques. Le modèle prendra en entrée une image d’un chantier (ou d’un environnement similaire), une liste d’objets détectés (principalement des personnes) à partir de capteurs préexistants, ainsi que des informations météorologiques. Le modèle générera ensuite un rapport sur les meilleures pratiques à adopter pour atténuer les risques.\n",
    "\n",
    "D’après les données, nous pouvons constater que pour chaque personne détectée, nous disposons d’un bounding box ainsi que de scores pour quatre critères de gestion des risques recherchés : port du casque, port du pantalon de sécurité, port du gilet de sécurité, et absence totale d’équipements de protection individuelle (PPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b915e5",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "### La première étape:\n",
    "Consiste à préparer les données pour l’entraînement ou le fine-tuning. Je vais commencer par mixer les deux dossiers d’images ainsi que les deux fichiers JSON. Je ne conserverai que les informations concernant les personnes ayant un score de détection d’objet supérieur ou égal à 0,5 :\n",
    "\n",
    "1) Pour réduire le nombre de requêtes envoyées à l’API d’OpenAI.\n",
    "2) L’étape de détection est considérée comme déjà entraînée et fine-tunée avant la génération du fichier JSON.\n",
    "\n",
    "Le nouveau fichier json **images_EST_GT.json** va servir comme 'Ground Truth' pour l’entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d05d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries que nous allons utiliser\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils.util import convert_image_to_base64, split_image, parse_llm_response\n",
    "from utils.util import get_predictions, get_ground_truth\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "768159f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/images_EST-1.json', 'r', encoding='utf-8') as file:\n",
    "    images_data_1 = json.load(file)['images']\n",
    "with open('../assets/images_EST-2.json', 'r', encoding='utf-8') as file:\n",
    "    images_data_2 = json.load(file)['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_score = 0.5\n",
    "ground_truth = dict()\n",
    "\n",
    "# Pour chaque image, nous allons vérifier les détections d'objets pour les personnes détectées.\n",
    "\n",
    "meta_data = {'has_high_vis_pant': 0, 'has_hard_hat': 0, 'has_high_vis_vest': 0, 'no_ppe': 0}\n",
    "for image_path, image_data in images_data_1.items():\n",
    "    detections = []\n",
    "    for detected_obj in image_data.get('detections', []):\n",
    "        if detected_obj['score'] >= accepted_score and detected_obj['label'] == 'person':\n",
    "            detections.append(detected_obj)\n",
    "            for key, val in detected_obj.get('attributes', {}).items():\n",
    "                if key in meta_data and val >= accepted_score:\n",
    "                    meta_data[key] += 1\n",
    "\n",
    "    if len(detections) > 0:\n",
    "        ground_truth[image_path] = {'detections': detections, 'timestamp':image_data['image_shooting']}\n",
    "    \n",
    "\n",
    "for image_path, image_data in images_data_2.items():\n",
    "    detections = []\n",
    "    for detected_obj in image_data.get('detections', []):\n",
    "        if detected_obj['score'] >= accepted_score and detected_obj['label'] == 'person':\n",
    "            detections.append(detected_obj)\n",
    "            for key, val in detected_obj.get('attributes', {}).items():\n",
    "                if key in meta_data and val >= accepted_score:\n",
    "                    meta_data[key] += 1\n",
    "    if len(detections) > 0:\n",
    "        ground_truth[image_path] = {'detections': detections, 'timestamp':image_data['image_shooting']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le résultat dans un fichier JSON.\n",
    "with open('../assets/images_EST_GT.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump({'images': ground_truth, 'meta-info': meta_data}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copier les images dans le dossier de Ground Truth\n",
    "for image in ground_truth:\n",
    "    try:\n",
    "        shutil.copyfile('../assets/images_EST-1/' + image, '../assets/images_EST_GT/' + image)\n",
    "    except Exception as _:\n",
    "        shutil.copyfile('../assets/images_EST-2/' + image, '../assets/images_EST_GT/' + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri des images par timestamp pour une meilleure organisation et qui va aider lors de l'entraînement avec les données de la météo.\n",
    "\n",
    "with open('../assets/images_EST_GT.json', 'r', encoding='utf-8') as file:\n",
    "    ground_truth = json.load(file)\n",
    "sorted_gt = dict(sorted(ground_truth['images'].items(), key=lambda item: item[1]['timestamp']))\n",
    "\n",
    "with open('../assets/images_EST_GT_sorted.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump({'images': sorted_gt, 'meta-info': ground_truth['meta-info']}, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910fb5ee",
   "metadata": {},
   "source": [
    "### La deuxième étape : \n",
    "Consiste à diviser les données en ensembles d’entraînement, de validation et de test:\n",
    "\n",
    "- Entraînement : nous pouvons utiliser différents paramètres pour le traitement des images avec les LLMs, par exemple : haute ou basse résolution, ajout d’une marge autour des bounding boxes, etc.\n",
    "- Validation : tester différents prompts et évaluer lesquels donnent les meilleurs résultats\n",
    "- Test : simuler l’arrivée de nouvelles images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6096b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation Test Split\n",
    "accepted_score = 0.5\n",
    "train_len = int(0.6 * len(sorted_gt))\n",
    "valid_len = int(0.8 * len(sorted_gt))\n",
    "\n",
    "train_images = dict()\n",
    "valid_images = dict()\n",
    "test_images = dict()\n",
    "\n",
    "# Les données meta pour assurer que nous avons un bon équilibre entre les ensembles d'entraînement, de validation et de test.\n",
    "train_meta_data = {'has_high_vis_pant': 0, 'has_hard_hat': 0, 'has_high_vis_vest': 0, 'no_ppe': 0}\n",
    "valid_meta_data = {'has_high_vis_pant': 0, 'has_hard_hat': 0, 'has_high_vis_vest': 0, 'no_ppe': 0}\n",
    "test_meta_data = {'has_high_vis_pant': 0, 'has_hard_hat': 0, 'has_high_vis_vest': 0, 'no_ppe': 0}\n",
    "\n",
    "for ind, img_path in enumerate(sorted_gt):\n",
    "    img_obj = sorted_gt[img_path]\n",
    "    if ind < train_len:\n",
    "        train_images[img_path] = img_obj\n",
    "        for detected_obj in img_obj.get('detections', []):\n",
    "            for key, val in detected_obj.get('attributes', {}).items():\n",
    "                if key in train_meta_data and val >= accepted_score:\n",
    "                    train_meta_data[key] += 1\n",
    "\n",
    "    elif ind < valid_len:\n",
    "        valid_images[img_path] = img_obj\n",
    "        for detected_obj in img_obj.get('detections', []):\n",
    "            for key, val in detected_obj.get('attributes', {}).items():\n",
    "                if key in valid_meta_data and val >= accepted_score:\n",
    "                    valid_meta_data[key] += 1\n",
    "    else:\n",
    "        test_images[img_path] = img_obj\n",
    "        for detected_obj in img_obj.get('detections', []):\n",
    "            for key, val in detected_obj.get('attributes', {}).items():\n",
    "                if key in test_meta_data and val >= accepted_score:\n",
    "                    test_meta_data[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les ensembles d'entraînement, de validation et de test dans des fichiers JSON séparés.\n",
    "with open('../assets/images_EST_GT_train.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump({'images': train_images, 'meta-info': train_meta_data}, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open('../assets/images_EST_GT_valid.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump({'images': valid_images, 'meta-info': valid_meta_data}, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open('../assets/images_EST_GT_test.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump({'images': test_images, 'meta-info': test_meta_data}, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6a426",
   "metadata": {},
   "source": [
    "Pour l’entraînement, les paramètres suivants seront utilisés :\n",
    "\n",
    "- Résolution de l’image : {high, low}\n",
    "- Bounding Box avec une marge de 0,05 de la taille d'image originale.\n",
    "- Prompt: **promt_1** a été utilisé durant l’entraînement en utilisant XML comme sorti.\n",
    "\n",
    "**NOTE** : Je n’ai pas utilisé beaucoup de paramètres afin de minimiser le nombre de requêtes envoyées à l’API d’OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = '''You are a construction safety inspection assistant.\n",
    "\n",
    "Your task is to detect the presence of PPE (personal protective equipment) on a person in the image. \n",
    "For each item below, return a detection of either 1 (yes) or 0 (no), and a confidence score between 0.0 and 1.0.\n",
    "\n",
    "You must respond **only** using the following XML format. Do not add explanations or extra text and finish always with </no-ppe></output>.\n",
    "\n",
    "<output>\n",
    "    <has-hard-hat>\n",
    "        <detection>{1 or 0}</detection>\n",
    "        <confidence>{0.0 - 1.0}</confidence>\n",
    "    </has-hard-hat>\n",
    "    <has-high-vis-pants>\n",
    "        <detection>{1 or 0}</detection>\n",
    "        <confidence>{0.0 - 1.0}</confidence>\n",
    "    </has-high-vis-pants>\n",
    "    <has-high-vis-vest>\n",
    "        <detection>{1 or 0}</detection>\n",
    "        <confidence>{0.0 - 1.0}</confidence>\n",
    "    </has-high-vis-vest>\n",
    "    <no-ppe>\n",
    "        <detection>{1 or 0}</detection>\n",
    "        <confidence>{0.0 - 1.0}</confidence>\n",
    "    </no-ppe>\n",
    "</output>'''\n",
    "\n",
    "\n",
    "llm = OpenAI(api_key=os.getenv('MY_OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c0a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_image(image, promt, resolution='high'):\n",
    "\n",
    "    response = llm.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a construction safety expert.'},\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {'type': 'text', 'text': promt},\n",
    "                        {\n",
    "                            'type': 'image_url',\n",
    "                            'image_url': {\n",
    "                                'url': f'data:image/jpeg;base64,{image}', 'detail': resolution\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=600,\n",
    "            )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df989f19",
   "metadata": {},
   "source": [
    "Pour chaque personne, nous découperons l’image selon sa boîte englobante, puis nous la soumettrons au LLM afin qu’il détecte les équipements requis, avec :\n",
    "\n",
    "- une prédiction binaire (1 ou 0),\n",
    "- un score de confiance (entre 0 et 1),\n",
    "- et, si le LLM échoue à générer une sortie satisfaisante, un compteur d’échecs sera incrémenté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3c79039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(file_path, out_file, prompt, resolution='high'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        train_images = json.load(file)\n",
    "\n",
    "    train_images = train_images['images']\n",
    "    \n",
    "    llm_fails = 0\n",
    "    new_images = []\n",
    "    for image_path, img_obj in train_images.items():\n",
    "        predictions = []\n",
    "        img = Image.open(os.path.join('../assets/images_EST_GT/', image_path))\n",
    "        for det_ind, d_obj in enumerate(img_obj['detections']):\n",
    "            print(image_path, det_ind, llm_fails)\n",
    "            cropped_image = split_image(img, d_obj['bounding_box_start_x'], d_obj['bounding_box_end_x'],\n",
    "                                        d_obj['bounding_box_start_y'], d_obj['bounding_box_end_y'])\n",
    "            cropped_img_64 = convert_image_to_base64(cropped_image)\n",
    "            try:\n",
    "                img_info = analyse_image(cropped_img_64, prompt, resolution=resolution)\n",
    "                pred = parse_llm_response(img_info)\n",
    "                predictions.append(pred)\n",
    "            except Exception as _:\n",
    "                predictions.append('-1')\n",
    "                llm_fails += 1\n",
    "            time.sleep(3)  # To avoid hitting rate limits\n",
    "        json_info = {\n",
    "            'timestamp': img_obj['timestamp'],\n",
    "            'image_path': image_path,\n",
    "            'detections': predictions\n",
    "        }\n",
    "        new_images.append(json_info)\n",
    "\n",
    "    with open(out_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump({'images': new_images, 'LLM Failure': llm_fails}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer l'entraînement pour les résolutions 'high' et 'low'\n",
    "for resolution in ['high', 'low']:\n",
    "    fit('../assets/images_EST_GT_train.json', \n",
    "        '../assets/image_predictions_train_' + resolution + '.json', \n",
    "        resolution=resolution)\n",
    "    print(f'Training completed for resolution: {resolution}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01785478",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/images_EST_GT_train.json', 'r', encoding='utf-8') as file:\n",
    "    train_images = json.load(file)\n",
    "with open('../assets/image_predictions_train_high.json', 'r', encoding='utf-8') as file:\n",
    "    train_images_high = json.load(file)\n",
    "with open('../assets/image_predictions_train_low.json', 'r', encoding='utf-8') as file:\n",
    "    train_images_low = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f10d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_hat_true = []\n",
    "has_hat_high = []\n",
    "has_hat_low = []\n",
    "\n",
    "no_ppe_true = []\n",
    "no_ppe_high = []\n",
    "no_ppe_low = []\n",
    "\n",
    "\n",
    "ground_truth = get_ground_truth(train_images)\n",
    "has_hat_true, no_ppe_true = ground_truth['has_hat'], ground_truth['no_ppe']\n",
    "high_predictions = get_predictions(train_images_high)\n",
    "has_hat_high, no_ppe_high = high_predictions['has_hat'], high_predictions['no_ppe']\n",
    "low_predictions = get_predictions(train_images_low)\n",
    "has_hat_low, no_ppe_low = low_predictions['has_hat'], low_predictions['no_ppe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa597f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAT_HIGH 0.878\n",
      "HAT_LOW 0.833\n",
      "PPE_HIGH 0.878\n",
      "PPE_LOW 0.833\n",
      "LLM Fails HIGH: 4\n",
      "LLM Fails LOW: 11\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy HAT_HIGH', round(accuracy_score(has_hat_true, has_hat_high), 3))\n",
    "print('Accuracy HAT_LOW', round(accuracy_score(has_hat_true, has_hat_low), 3))\n",
    "print('Accuracy PPE_HIGH', round(accuracy_score(no_ppe_true, no_ppe_high), 3))\n",
    "print('Accuracy PPE_LOW', round(accuracy_score(no_ppe_true, no_ppe_low), 3))\n",
    "\n",
    "print('LLM Fails HIGH:', train_images_high['LLM Failure'])\n",
    "print('LLM Fails LOW:', train_images_low['LLM Failure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf448299",
   "metadata": {},
   "source": [
    "D’après les résultats précédents, l’utilisation d’images en haute résolution donne de meilleurs résultats. \n",
    "\n",
    "Ainsi, pour la validation des prompts, nous allons uniquement utiliser resolution=\"high\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a868bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une autre prompt pour la validation des prédictions LLM avec plus de détails et d'etre plus explicite sur les attentes.\n",
    "\n",
    "prompt_2 = '''You are an AI safety inspector. You are shown a construction site image.\n",
    "\n",
    "            Your only task is to check if the person in the image is wearing any of the following PPE (personal protective equipment):\n",
    "\n",
    "            - A hard hat\n",
    "            - A high-visibility vest\n",
    "            - High-visibility pants\n",
    "            - No PPE at all\n",
    "\n",
    "            Only include items in the output if the model's confidence is greater than 0.5.  \n",
    "            If confidence is ≤ 0.5, set `<prediction>` to 0 and still include the actual confidence value.\n",
    "            Predictions must be based only on visual evidence in the image. \n",
    "            Do not assume PPE is present unless it is clearly visible with confidence > 0.5.\n",
    "            Please respond using **only** the following XML structure. Do not include any extra text, explanation, or summaries. \n",
    "            Return only the content starting with <output> and ending with </output>.\n",
    "\n",
    "            <output>\n",
    "                <has-hard-hat>\n",
    "                    <prediction>{1 or 0}</prediction>\n",
    "                    <confidence>{0.0 - 1.0}</confidence>\n",
    "                </has-hard-hat>\n",
    "                <has-high-vis-pants>\n",
    "                    <prediction>{1 or 0}</prediction>\n",
    "                    <confidence>{0.0 - 1.0}</confidence>\n",
    "                </has-high-vis-pants>\n",
    "                <has-high-vis-vest>\n",
    "                    <prediction>{1 or 0}</prediction>\n",
    "                    <confidence>{0.0 - 1.0}</confidence>\n",
    "                </has-high-vis-vest>\n",
    "                <no-ppe>\n",
    "                    <prediction>{1 or 0}</prediction>\n",
    "                    <confidence>{0.0 - 1.0}</confidence>\n",
    "                </no-ppe>\n",
    "            </output>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e0da8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663679906_5f1ef703-d7d7-4a18-809c-23b38679e1dd.jpg 0 0\n",
      "664003659_e0e08173-7be7-4c02-8f53-675979c55e1d.jpg 0 0\n",
      "664225860_219a5555-e487-49ca-850b-5a6974218ef3.jpg 0 0\n",
      "664242924_1ca16733-6c77-4ed7-8d7a-06b9fe00b15b.jpg 0 0\n",
      "664495381_42ecdef9-0bc2-40d1-86a0-c5dcaba83d68.jpg 0 0\n",
      "664993276_ee64f700-1113-4170-a6bf-a5aeffcd726c.jpg 0 0\n",
      "665241409_6d476db8-b162-4cfc-a446-e3987bddfca2.jpg 0 0\n",
      "665769639_99620405-fc20-4dfd-bfd3-09d89ae748fd.jpg 0 0\n",
      "666071019_a10b59d1-b209-4265-81a6-a377dd518797.jpg 0 0\n",
      "666272086_7f8f06b3-a487-4161-95a8-0996f28242bd.jpg 0 0\n",
      "667412533_ffc12494-bdb0-4652-8722-cdf4a5e57ef1.jpg 0 0\n",
      "667429221_2feb1954-8caf-4f7c-919c-42e074ede61d.jpg 0 0\n",
      "667429221_2feb1954-8caf-4f7c-919c-42e074ede61d.jpg 1 0\n",
      "667666878_267fbc68-04ef-471c-a091-bfadf291b157.jpg 0 0\n",
      "668210461_f9792838-213a-41e5-9e2d-69be22648a01.jpg 0 0\n",
      "668210461_f9792838-213a-41e5-9e2d-69be22648a01.jpg 1 0\n",
      "668223583_d0b3f8e3-d103-4527-8308-afca6d6a69a0.jpg 0 0\n",
      "668223583_d0b3f8e3-d103-4527-8308-afca6d6a69a0.jpg 1 0\n",
      "668745738_9468ddb1-b33f-4346-9f01-1abbe9ef15b0.jpg 0 0\n",
      "668984783_b620f3f7-5386-4c66-bcf6-9886852638b9.jpg 0 0\n",
      "668984783_b620f3f7-5386-4c66-bcf6-9886852638b9.jpg 1 0\n",
      "668984783_b620f3f7-5386-4c66-bcf6-9886852638b9.jpg 2 0\n",
      "669219149_d48d3599-d66b-4f8a-b695-f16e40b65f53.jpg 0 0\n",
      "669445536_33a03be1-1130-40d9-a667-2a24e99f040d.jpg 0 0\n",
      "669445536_33a03be1-1130-40d9-a667-2a24e99f040d.jpg 1 0\n",
      "669469833_f70fc7d8-b807-4f1e-815a-d8359b57a47d.jpg 0 0\n",
      "669480818_e3c5fb10-ac7d-4d38-8f0b-1ec62911fd92.jpg 0 0\n",
      "669480818_e3c5fb10-ac7d-4d38-8f0b-1ec62911fd92.jpg 1 0\n",
      "669480818_e3c5fb10-ac7d-4d38-8f0b-1ec62911fd92.jpg 2 0\n",
      "669481131_5cd0818d-f5f0-44b5-971a-b81a6a9de5ab.jpg 0 0\n",
      "669481131_5cd0818d-f5f0-44b5-971a-b81a6a9de5ab.jpg 1 1\n",
      "669526659_6e126e9f-df10-4094-88a0-2d3aeb7a783e.jpg 0 1\n",
      "669526659_6e126e9f-df10-4094-88a0-2d3aeb7a783e.jpg 1 2\n",
      "669581266_6efc5251-5bd3-4755-a86a-f6933dfa3476.jpg 0 2\n",
      "670227370_a48f9130-8685-4b78-826b-02c4ab44da9a.jpg 0 2\n",
      "670227370_a48f9130-8685-4b78-826b-02c4ab44da9a.jpg 1 2\n",
      "670227370_a48f9130-8685-4b78-826b-02c4ab44da9a.jpg 2 2\n",
      "670216838_f8e6fa8f-c941-4f46-b030-4d7b0cc087ad.jpg 0 2\n",
      "670283200_7c973646-0269-49ff-aa52-d367995620cc.jpg 0 2\n",
      "670282846_357523c4-ce89-4ff8-82d1-431a06c405f5.jpg 0 2\n",
      "Validation completed for prompt: 1\n",
      "663679906_5f1ef703-d7d7-4a18-809c-23b38679e1dd.jpg 0 0\n",
      "664003659_e0e08173-7be7-4c02-8f53-675979c55e1d.jpg 0 0\n",
      "664225860_219a5555-e487-49ca-850b-5a6974218ef3.jpg 0 0\n",
      "664242924_1ca16733-6c77-4ed7-8d7a-06b9fe00b15b.jpg 0 0\n",
      "664495381_42ecdef9-0bc2-40d1-86a0-c5dcaba83d68.jpg 0 0\n",
      "664993276_ee64f700-1113-4170-a6bf-a5aeffcd726c.jpg 0 0\n",
      "665241409_6d476db8-b162-4cfc-a446-e3987bddfca2.jpg 0 0\n",
      "665769639_99620405-fc20-4dfd-bfd3-09d89ae748fd.jpg 0 0\n",
      "666071019_a10b59d1-b209-4265-81a6-a377dd518797.jpg 0 0\n",
      "666272086_7f8f06b3-a487-4161-95a8-0996f28242bd.jpg 0 0\n",
      "667412533_ffc12494-bdb0-4652-8722-cdf4a5e57ef1.jpg 0 0\n",
      "667429221_2feb1954-8caf-4f7c-919c-42e074ede61d.jpg 0 0\n",
      "667429221_2feb1954-8caf-4f7c-919c-42e074ede61d.jpg 1 0\n",
      "667666878_267fbc68-04ef-471c-a091-bfadf291b157.jpg 0 0\n",
      "668210461_f9792838-213a-41e5-9e2d-69be22648a01.jpg 0 0\n",
      "668210461_f9792838-213a-41e5-9e2d-69be22648a01.jpg 1 0\n",
      "668223583_d0b3f8e3-d103-4527-8308-afca6d6a69a0.jpg 0 0\n",
      "668223583_d0b3f8e3-d103-4527-8308-afca6d6a69a0.jpg 1 0\n",
      "668745738_9468ddb1-b33f-4346-9f01-1abbe9ef15b0.jpg 0 0\n",
      "668984783_b620f3f7-5386-4c66-bcf6-9886852638b9.jpg 0 0\n",
      "668984783_b620f3f7-5386-4c66-bcf6-9886852638b9.jpg 1 0\n",
      "668984783_b620f3f7-5386-4c66-bcf6-9886852638b9.jpg 2 0\n",
      "669219149_d48d3599-d66b-4f8a-b695-f16e40b65f53.jpg 0 0\n",
      "669445536_33a03be1-1130-40d9-a667-2a24e99f040d.jpg 0 0\n",
      "669445536_33a03be1-1130-40d9-a667-2a24e99f040d.jpg 1 0\n",
      "669469833_f70fc7d8-b807-4f1e-815a-d8359b57a47d.jpg 0 0\n",
      "669480818_e3c5fb10-ac7d-4d38-8f0b-1ec62911fd92.jpg 0 0\n",
      "669480818_e3c5fb10-ac7d-4d38-8f0b-1ec62911fd92.jpg 1 0\n",
      "669480818_e3c5fb10-ac7d-4d38-8f0b-1ec62911fd92.jpg 2 0\n",
      "669481131_5cd0818d-f5f0-44b5-971a-b81a6a9de5ab.jpg 0 0\n",
      "669481131_5cd0818d-f5f0-44b5-971a-b81a6a9de5ab.jpg 1 0\n",
      "669526659_6e126e9f-df10-4094-88a0-2d3aeb7a783e.jpg 0 0\n",
      "669526659_6e126e9f-df10-4094-88a0-2d3aeb7a783e.jpg 1 1\n",
      "669581266_6efc5251-5bd3-4755-a86a-f6933dfa3476.jpg 0 1\n",
      "670227370_a48f9130-8685-4b78-826b-02c4ab44da9a.jpg 0 1\n",
      "670227370_a48f9130-8685-4b78-826b-02c4ab44da9a.jpg 1 1\n",
      "670227370_a48f9130-8685-4b78-826b-02c4ab44da9a.jpg 2 1\n",
      "670216838_f8e6fa8f-c941-4f46-b030-4d7b0cc087ad.jpg 0 1\n",
      "670283200_7c973646-0269-49ff-aa52-d367995620cc.jpg 0 1\n",
      "670282846_357523c4-ce89-4ff8-82d1-431a06c405f5.jpg 0 1\n",
      "Validation completed for prompt: 2\n"
     ]
    }
   ],
   "source": [
    "# Lancer l'entraînement pour les résolutions 'high' et 'low'\n",
    "prompts = [prompt_1, prompt_2]\n",
    "for ind, prompt in enumerate(prompts):\n",
    "    fit('../assets/images_EST_GT_valid.json', \n",
    "        '../assets/image_predictions_valid_prompt_' + str(ind+1) + '.json', prompt=prompt,\n",
    "        resolution=resolution)\n",
    "    print(f'Validation completed for prompt: {ind+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8472d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/images_EST_GT_valid.json', 'r', encoding='utf-8') as file:\n",
    "    valid_images = json.load(file)\n",
    "with open('../assets/image_predictions_valid_prompt_1.json', 'r', encoding='utf-8') as file:\n",
    "    valid_images_prompt_1 = json.load(file)\n",
    "with open('../assets/image_predictions_valid_prompt_2.json', 'r', encoding='utf-8') as file:\n",
    "    valid_images_prompt_2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca43b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_hat_true = []\n",
    "has_hat_prompt_1 = []\n",
    "has_hat_prompt_2 = []\n",
    "\n",
    "no_ppe_true = []\n",
    "no_ppe_prompt_1 = []\n",
    "no_ppe_prompt_2 = []\n",
    "\n",
    "ground_truth = get_ground_truth(valid_images)\n",
    "has_hat_true, no_ppe_true = ground_truth['has_hat'], ground_truth['no_ppe']\n",
    "high_predictions = get_predictions(valid_images_prompt_1)\n",
    "has_hat_prompt_1, no_ppe_prompt_1 = high_predictions['has_hat'], high_predictions['no_ppe']\n",
    "low_predictions = get_predictions(valid_images_prompt_2)\n",
    "has_hat_prompt_2, no_ppe_prompt_2 = low_predictions['has_hat'], low_predictions['no_ppe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAT_PROMPT_1 0.85\n",
      "HAT_PROMPT_2 0.875\n",
      "PPE_PROMPT_1 0.85\n",
      "PPE_PROMPT_2 0.875\n",
      "LLM Fails PROMPT_1: 2\n",
      "LLM Fails PROMPT_2: 1\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy HAT_PROMPT_1', round(accuracy_score(has_hat_true, has_hat_prompt_1), 3))\n",
    "print('Accuracy HAT_PROMPT_2', round(accuracy_score(has_hat_true, has_hat_prompt_2), 3))\n",
    "print('Accuracy PPE_PROMPT_1', round(accuracy_score(no_ppe_true, no_ppe_prompt_1), 3))\n",
    "print('Accuracy PPE_PROMPT_2', round(accuracy_score(no_ppe_true, no_ppe_prompt_2), 3))\n",
    "\n",
    "print('LLM Fails PROMPT_1:', valid_images_prompt_1['LLM Failure'])\n",
    "print('LLM Fails PROMPT_2:', valid_images_prompt_2['LLM Failure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f535e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670484413_30a9e6a7-1a81-4f83-89d2-d14f5093f4e1.jpg 0 0\n",
      "670727037_f607ad68-c8bb-4b3f-a477-65ada65153f3.jpg 0 0\n",
      "671945586_f6bfc504-4033-4c0f-b5ab-c1bed8b9d62e.jpg 0 0\n",
      "671966647_270f53ad-9584-4ed9-9dfb-8e7b25da322d.jpg 0 0\n",
      "671990197_90b445bd-97dd-4fea-b6b4-7fa17fbbf3fb.jpg 0 0\n",
      "671999665_23d821e2-7359-41b1-9ae7-aa2fb43ef604.jpg 0 0\n",
      "671999665_23d821e2-7359-41b1-9ae7-aa2fb43ef604.jpg 1 0\n",
      "672014395_d583f064-4851-46f7-9276-c0fd345dafe0.jpg 0 0\n",
      "672014395_d583f064-4851-46f7-9276-c0fd345dafe0.jpg 1 0\n",
      "672014395_d583f064-4851-46f7-9276-c0fd345dafe0.jpg 2 0\n",
      "672014347_2eeb042f-86c5-4724-a4a1-b674aa0be21c.jpg 0 0\n",
      "672014347_2eeb042f-86c5-4724-a4a1-b674aa0be21c.jpg 1 0\n",
      "672014347_2eeb042f-86c5-4724-a4a1-b674aa0be21c.jpg 2 0\n",
      "672027488_bde0f884-99fb-4783-8183-243bc1801962.jpg 0 0\n",
      "672045392_087ebdab-5484-4888-bd09-f9a15cc4339b.jpg 0 0\n",
      "672045974_af43b6db-6224-4ee4-afaa-3ac5afa71e6a.jpg 0 0\n",
      "672063142_3d1bcb4a-c78f-4df5-9d44-dbd4c0b64242.jpg 0 0\n",
      "672092854_f52b37a0-a30c-4d60-a18c-4da2afdedc12.jpg 0 0\n",
      "672092854_f52b37a0-a30c-4d60-a18c-4da2afdedc12.jpg 1 0\n",
      "672092733_f6aa6883-9a8a-4d4a-a012-e7ad6a306ac7.jpg 0 0\n",
      "672128810_80cf2b5f-8af8-49b8-b469-5cfa77d2c340.jpg 0 1\n",
      "672127076_0abb3c22-9b00-43d4-ba8d-48184ad57c20.jpg 0 1\n",
      "672127076_0abb3c22-9b00-43d4-ba8d-48184ad57c20.jpg 1 1\n",
      "672127076_0abb3c22-9b00-43d4-ba8d-48184ad57c20.jpg 2 1\n",
      "672155134_72810a13-fb77-42fb-8da5-23589429393a.jpg 0 1\n",
      "672155134_72810a13-fb77-42fb-8da5-23589429393a.jpg 1 1\n",
      "672155134_72810a13-fb77-42fb-8da5-23589429393a.jpg 2 1\n",
      "672269385_94c762db-768b-4cd6-955c-f5df99203f91.jpg 0 1\n",
      "672286586_4c9d0e58-d730-4a12-8c0b-663c88143cbf.jpg 0 1\n",
      "672286586_4c9d0e58-d730-4a12-8c0b-663c88143cbf.jpg 1 1\n",
      "672306685_3940e471-f25d-4c49-8776-e782eb6bd99b.jpg 0 1\n",
      "672327869_183d472a-7e1e-4e96-ade8-4c592a0ccf26.jpg 0 1\n",
      "672384818_d3155c53-1ac2-4a15-bd12-83bc8fe5099b.jpg 0 1\n",
      "672384818_d3155c53-1ac2-4a15-bd12-83bc8fe5099b.jpg 1 1\n",
      "672790132_5fdaa5be-747d-4603-b696-1471bbf57608.jpg 0 1\n",
      "672811514_b84dc26e-1da4-40ce-98cf-0191d31475fa.jpg 0 1\n",
      "672811514_b84dc26e-1da4-40ce-98cf-0191d31475fa.jpg 1 1\n",
      "673057027_c41fa684-60aa-4d65-a792-54e204457c4f.jpg 0 1\n",
      "673519099_92de7c4c-5554-40c9-835f-662dd4284ffe.jpg 0 1\n",
      "673519376_ffa74176-7e11-4eb1-a274-0364bd66813d.jpg 0 1\n",
      "673760999_3dcaeed0-3c71-4dce-bd7f-a896e3e647cb.jpg 0 1\n",
      "673760999_3dcaeed0-3c71-4dce-bd7f-a896e3e647cb.jpg 1 1\n",
      "Testing completed\n"
     ]
    }
   ],
   "source": [
    "# Lancer l'entraînement pour les résolutions 'high' et 'low'\n",
    "fit('../assets/images_EST_GT_test.json', \n",
    "    '../assets/image_predictions_test.json', prompt=prompt_2,\n",
    "    resolution='high')\n",
    "print(f'Testing completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9f3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/images_EST_GT_test.json', 'r', encoding='utf-8') as file:\n",
    "    test_images = json.load(file)\n",
    "with open('../assets/image_predictions_test.json', 'r', encoding='utf-8') as file:\n",
    "    test_predictions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f687145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy HAT_TEST 0.81\n",
      "Accuracy PPE_TEST 0.786\n",
      "LLM Fails TEST: 1\n"
     ]
    }
   ],
   "source": [
    "has_hat_true = []\n",
    "has_hat_test = []\n",
    "\n",
    "no_ppe_true = []\n",
    "no_ppe_test = []\n",
    "\n",
    "ground_truth = get_ground_truth(test_images)\n",
    "has_hat_true, no_ppe_true = ground_truth['has_hat'], ground_truth['no_ppe']\n",
    "predictions = get_predictions(test_predictions)\n",
    "has_hat_test, no_ppe_test = predictions['has_hat'], predictions['no_ppe']\n",
    "\n",
    "print('Accuracy HAT_TEST', round(accuracy_score(has_hat_true, has_hat_test), 3))\n",
    "print('Accuracy PPE_TEST', round(accuracy_score(no_ppe_true, no_ppe_test), 3))\n",
    "\n",
    "print('LLM Fails TEST:', test_predictions['LLM Failure'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enlaps_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
